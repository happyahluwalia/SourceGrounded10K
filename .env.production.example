# Production Environment Configuration
# Copy this to .env and update with your actual values

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================
APP_NAME=10kiq
DOMAIN=10kiq.com
DEBUG=false
LOG_LEVEL=INFO
ENABLE_DEBUG_LOGS=false

# Frontend API URL (use relative path for same-domain deployment)
VITE_API_URL=/api

# ============================================================================
# SECURITY
# ============================================================================
# Comma-separated list of allowed origins (auto-configured from DOMAIN)
CORS_ORIGINS=https://10kiq.com,https://www.10kiq.com,http://localhost:3000

# API key for authentication (currently not implemented - reserved for future use)
# Will be auto-generated by gpu_deploy.sh
API_KEY=your_secure_api_key_here_change_this

# Rate limiting
RATE_LIMIT_PER_MINUTE=10
RATE_LIMIT_PER_HOUR=100

# ============================================================================
# DATABASE - PostgreSQL
# ============================================================================
DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/financeagent

# Postgres container settings
POSTGRES_DB=financeagent
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password_here_change_this

# ============================================================================
# VECTOR DATABASE - Qdrant
# ============================================================================
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=financial_documents

# ============================================================================
# CACHE - Redis
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# ============================================================================
# LLM - Ollama (v2 Architecture)
# ============================================================================
OLLAMA_BASE_URL=http://ollama:11434

# V2 Architecture uses 3 specialized models:
# - Supervisor: Routes queries to appropriate tools
# - Planner: Creates structured execution plans
# - Synthesizer: Generates final answers from context
#
# PRODUCTION RECOMMENDATION:
# Use llama3.1 for all three for better quality (~6GB total, requires 16GB+ RAM)
#
# ALTERNATIVE (if you have 16GB+ RAM):
# Use llama3.1:8b for Supervisor/Planner for better reasoning

VLLM_MODEL=meta-llama/Llama-3.2-3B-Instruct
SUPERVISOR_MODEL=Llama-3.2-3B-Instruct
PLANNER_MODEL=Llama-3.2-3B-Instruct
SYNTHESIZER_MODEL=Llama-3.2-3B-Instruct

# ============================================================================
# EMBEDDINGS
# ============================================================================
# Production embedding model
# nomic-embed-text: 768-dim, 8K context window, optimized for long docs
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSION=768

# ============================================================================
# CHUNKING STRATEGY
# ============================================================================
# Context window math for phi3 (4K tokens):
# - 5 chunks × 1024 chars/chunk = 5,120 chars
# - 5,120 chars ÷ 4 chars/token ≈ 1,280 tokens (31% of context)
# - Prompt overhead: ~200 tokens
# - Answer: 500 tokens
# - Total: ~1,980 tokens (48% utilization, leaves 2K buffer)
CHUNK_SIZE=1024
CHUNK_OVERLAP=150

# ============================================================================
# RAG SETTINGS
# ============================================================================
TOP_K=5                    # Number of chunks to retrieve from Qdrant
SCORE_THRESHOLD=0.5        # Minimum similarity score (0-1)
MAX_TOKENS=500             # Maximum tokens in LLM response

# ============================================================================
# BATCH PROCESSING
# ============================================================================
EMBEDDING_BATCH_SIZE=32           # Batch size for embedding generation
QDRANT_UPLOAD_BATCH_SIZE=100      # Batch size for Qdrant uploads

# ============================================================================
# SEC EDGAR API
# ============================================================================
# Required by SEC: Format "FirstName LastName email@domain.com"
# SEC will block requests without proper User-Agent
SEC_USER_AGENT=YourFirstName YourLastName your.email@example.com

# ============================================================================
# HuggingFace Token
# ============================================================================
# Used by vLLM to download models from hugging face
# Get your token from https://huggingface.co/settings/tokens with read access to repos
HF_TOKEN=Your token

# ============================================================================
# NOTES FOR DEPLOYMENT
# ============================================================================
# 
# 1. BEFORE DEPLOYING:
#    - Change all passwords and API keys
#    - Update CORS_ORIGINS with your actual domain
#    - Update SEC_USER_AGENT with your real name and email
#
# 2. PULL MODELS FIRST:
#    docker exec -it financeagent_ollama ollama pull nomic-embed-text
#    docker exec -it financeagent_ollama ollama pull phi3:mini-instruct
#    # Note: Only need to pull phi3:mini-instruct once since all 3 agents use it
#
# 3. RECREATE QDRANT COLLECTION (dimension changed 1024 → 768):
#    curl -X DELETE http://localhost:6333/collections/financial_documents
#    docker-compose restart app
#
# 4. MEMORY MONITORING (8GB Droplet):
#    - Phi-3: ~3GB (safe, recommended)
#    - Llama 3.1: ~6GB (monitor closely)
#    - Total system: should stay under 6GB
#
# 5. VERIFY SETUP:
#    docker exec -it financeagent_app python scripts/verify_production_models.py
#
# ============================================================================
# Langsmith settings for observability
export LANGCHAIN_TRACING_V2=True
export LANGCHAIN_API_KEY=
