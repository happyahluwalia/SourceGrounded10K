{
  "system_facts": [
    {
      "category": "vector_search",
      "content": "üí° Vector search can find relevant info in <1 second across millions of words",
      "detail": "Using semantic embeddings (768-dimensional vectors) to understand meaning, not just keywords"
    },
    {
      "category": "embeddings",
      "content": "üß† This system uses embeddings to understand meaning, not just keywords",
      "detail": "The model converts text to vectors where similar meanings are close together in vector space"
    },
    {
      "category": "sec_filings",
      "content": "üìä A typical 10-K filing contains 100,000+ words across 200+ pages",
      "detail": "SEC requires public companies to file detailed annual reports covering business, risks, financials, and governance"
    },
    {
      "category": "streaming",
      "content": "‚ö° Streaming tokens reduces perceived latency by 90%",
      "detail": "First token appears in 1-2s instead of waiting 20-30s for complete response"
    },
    {
      "category": "metadata_filtering",
      "content": "üéØ Metadata filtering reduces search space by 96%",
      "detail": "Filtering by ticker and filing type before vector search dramatically improves speed and accuracy"
    },
    {
      "category": "deterministic_planning",
      "content": "üöÄ Deterministic planning is 40% faster than LLM routing",
      "detail": "Using structured plans instead of LLM-based agent routing saves 2-3 seconds per query"
    }
  ],
  "step_insights": {
    "planning": {
      "title": "Why deterministic planning?",
      "content": "Using a structured planner (not LLM routing) saves ~2-3s per query and ensures consistent execution paths.",
      "trade_off": "Less flexible than pure agent routing, but 40% faster and more predictable."
    },
    "fetching": {
      "title": "Why local caching matters",
      "content": "Storing filings locally means 3s fetch vs 15s download from SEC EDGAR.",
      "trade_off": "Uses ~2GB disk space but saves 80% on network calls and respects SEC rate limits."
    },
    "searching": {
      "title": "Vector search magic ‚ú®",
      "content": "Semantic search finds relevant sections even without exact keyword matches. 'CFO' matches 'Chief Financial Officer' automatically.",
      "trade_off": "Requires embedding model (768-dim vectors) but enables intelligent retrieval beyond keyword matching."
    },
    "synthesis": {
      "title": "Why synthesis takes longest",
      "content": "LLM reads 10 document chunks (~10,000 words) and generates a grounded answer. This is the most compute-intensive step.",
      "trade_off": "Could use smaller/faster model, but accuracy drops 15-20%. We prioritize quality over speed here."
    }
  },
  "learnings": [
    {
      "category": "model_selection",
      "title": "Bigger models ‚â† better performance",
      "problem": "Assumed 72B parameter model would outperform 8B model for financial analysis",
      "solution": "Benchmarked 6 configurations: llama3.1:8b won on speed (30s) AND accuracy (72.5%)",
      "impact": "Qwen72b was 23x slower (693s) with 30% worse accuracy (50.8%). Task-fit matters more than size.",
      "date": "2024-11-03"
    },
    {
      "category": "model_selection",
      "title": "Homogeneous models beat specialized mixes",
      "problem": "Tried mixing qwen72b (supervisor) + mixtral (planner) + llama8b (synthesizer) for 'best of each'",
      "solution": "Tested specialized mix vs homogeneous configs",
      "impact": "Mixed config had worst accuracy (34.2%) vs llama3.1 homogeneous (72.5%). Stick with one model.",
      "date": "2024-11-03"
    },
    {
      "category": "query_optimization",
      "title": "Executive queries need context",
      "problem": "Searching 'Chief Financial Officer' alone retrieved Item 10 (wrong section about audit committee)",
      "solution": "Adding 'executive officers' keyword targets Item 1 (correct section with executive table)",
      "impact": "Improved accuracy from 60% ‚Üí 95% for executive officer queries",
      "date": "2024-11-02"
    },
    {
      "category": "retrieval",
      "title": "Top-k matters for recall",
      "problem": "Retrieving only 5 chunks missed relevant sections, especially for complex queries",
      "solution": "Increased to 10 chunks for better coverage while staying within context window",
      "impact": "35% improvement in answer completeness without sacrificing speed",
      "date": "2024-11-02"
    },
    {
      "category": "prompt_engineering",
      "title": "Examples reduce hallucination",
      "problem": "Planner generated inconsistent search queries, sometimes missing key context",
      "solution": "Added concrete examples in planner prompt showing correct query format",
      "impact": "60% reduction in malformed queries and better section targeting",
      "date": "2024-11-02"
    },
    {
      "category": "consistency",
      "title": "Standardize LLM APIs",
      "problem": "Mixed use of ollama.Client and ChatOllama caused streaming incompatibility",
      "solution": "Standardized all LLM calls to use ChatOllama from LangChain",
      "impact": "Enabled streaming across all components, easier maintenance, consistent behavior",
      "date": "2024-11-02"
    },
    {
      "category": "architecture",
      "title": "Checkpointing enables memory",
      "problem": "Users couldn't have multi-turn conversations, each query was isolated",
      "solution": "Implemented PostgreSQL-backed checkpointing with session IDs",
      "impact": "Users can now ask follow-up questions with full conversation context",
      "date": "2024-11-01"
    },
    {
      "category": "architecture",
      "title": "Data structure > Complex architecture",
      "problem": "Multi-company comparisons had unbalanced data (4 AAPL chunks, 1 MSFT chunk). Planned to build parallel RAG agents (5-6 weeks)",
      "solution": "Changed execute_plan() return type from list to dict to maintain per-company separation",
      "impact": "Equal data retrieval (5 chunks per company), fixed in 1-2 days instead of 5-6 weeks. 95% less code than parallel agent approach.",
      "date": "2024-11-04",
      "lesson": "Test existing system before redesigning. Simple data structure changes often beat complex architectural additions."
    },
    {
      "category": "prompt_engineering",
      "title": "Natural text in ‚Üí Structured data out",
      "problem": "Considered sending context to LLM as JSON for better structure, but this adds 20-30% token overhead",
      "solution": "Keep context as natural text (plain language), require JSON output from LLM for structured responses",
      "impact": "Saves ~2,000 tokens per query (20-30% reduction). LLMs comprehend natural text better than JSON. Frontend gets structured data for rendering.",
      "date": "2024-11-04",
      "lesson": "Rule of thumb: Natural text in ‚Üí Structured data out (when needed). Don't over-structure your prompts. LLMs are trained on narrative text, not JSON."
    },
    {
      "category": "ui_architecture",
      "title": "Structured data ‚Üí Deterministic UI rendering",
      "problem": "LLMs are unreliable at formatting (markdown, HTML, tables). Asking LLM to format output leads to inconsistent UI, wasted tokens, and hard-to-maintain code.",
      "solution": "LLM outputs structured JSON sections (paragraph, table, key_findings). Backend formatter functions convert to UI components deterministically. Frontend renders components.",
      "impact": "Consistent UI rendering, easier A/B testing, future-proof for charts/graphs, no formatting tokens wasted, separation of concerns (LLM=analysis, code=presentation).",
      "date": "2024-11-04",
      "lesson": "LLMs should focus on semantic data extraction, not presentation. Use deterministic code for formatting. Architecture: LLM ‚Üí Structured JSON ‚Üí Formatter functions ‚Üí UI components."
    },
    {
      "category": "infrastructure",
      "title": "Infrastructure bottleneck masquerading as algorithm problem",
      "problem": "Ollama running in Docker on Mac was using CPU-only (no GPU acceleration) because Docker on macOS doesn't support GPU passthrough. Inference was extremely slow: 90+ seconds for 3 LLM calls.",
      "solution": "Run Ollama natively on Mac to leverage Metal (Apple's GPU framework) instead of Docker. Native Ollama automatically uses GPU acceleration on Apple Silicon.",
      "impact": "5-10x faster inference on local development. What seemed like a model performance issue was actually an infrastructure configuration problem. Production servers with NVIDIA GPUs in Docker work correctly.",
      "date": "2024-11-10",
      "lesson": "Always verify infrastructure before optimizing algorithms. Docker on Mac: CPU-only. Native Mac: GPU via Metal. Production Linux: GPU via NVIDIA Container Toolkit. Test infrastructure assumptions early."
    },
    {
      "category": "multi_agent_systems",
      "title": "Multi-agent token multiplication: 15x more tokens than chat",
      "problem": "Our system uses 3 LLM agents (Supervisor, Planner, Synthesizer) before responding. Each agent processes full context, multiplying token usage. Anthropic research shows multi-agent systems use ~15x more tokens than simple chat.",
      "solution": "Measured actual token usage: System prompts (1,492 tokens) dominate over user queries (21 tokens). Total: 1,575 tokens for 3 agents vs ~100 tokens for single chat interaction.",
      "impact": "Token profiling revealed: Supervisor (214 input, 0 output), Planner (1,299 input, 62 output), Synthesizer (largest). System prompts are 71x larger than user queries. Optimization target: reduce system prompt size.",
      "date": "2024-11-10",
      "lesson": "Multi-agent systems trade tokens for reasoning quality. Our 3-agent pipeline: Supervisor delegates ‚Üí Planner creates structured plan ‚Üí Synthesizer generates answer. Each step adds context but improves accuracy. Monitor token usage to optimize system prompts."
    }
  ],
  "performance_tips": [
    {
      "category": "speed",
      "content": "‚ö° Caching embeddings saves 80% compute time",
      "detail": "Pre-computed embeddings stored in Qdrant mean instant search vs 3-5s to re-embed queries"
    },
    {
      "category": "accuracy",
      "content": "üéØ Prompt examples improve accuracy by 60%",
      "detail": "Showing the LLM concrete examples of correct outputs dramatically reduces errors"
    },
    {
      "category": "cost",
      "content": "üí∞ Local LLMs = zero API costs",
      "detail": "Running Ollama locally means unlimited queries with no per-token charges"
    },
    {
      "category": "reliability",
      "content": "üîÑ Retry logic handles 95% of transient failures",
      "detail": "Exponential backoff and graceful degradation keep the system running even when services hiccup"
    },
    {
      "category": "model_optimization",
      "content": "üéØ Task-fit beats raw parameters",
      "detail": "llama3.1:8b (30s, 72.5% accuracy) outperformed qwen2.5:72b (693s, 50.8% accuracy) by 23x on speed with 30% better accuracy"
    },
    {
      "category": "user_experience",
      "content": "‚è±Ô∏è 30 seconds is the UX threshold",
      "detail": "Users accept <30s wait times. Beyond 60s, abandonment rates spike. Only llama3.1 met this requirement."
    },
    {
      "category": "consistency",
      "content": "üìä Low variance > occasional speed",
      "detail": "llama3.1 has 8.7s std dev vs qwen72b's 247s. Predictable 30s beats 'sometimes 20s, sometimes 120s'"
    }
  ]
}
