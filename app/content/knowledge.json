{
  "system_facts": [
    {
      "category": "vector_search",
      "content": "ðŸ’¡ Vector search can find relevant info in <1 second across millions of words",
      "detail": "Using semantic embeddings (768-dimensional vectors) to understand meaning, not just keywords"
    },
    {
      "category": "embeddings",
      "content": "ðŸ§  This system uses embeddings to understand meaning, not just keywords",
      "detail": "The model converts text to vectors where similar meanings are close together in vector space"
    },
    {
      "category": "sec_filings",
      "content": "ðŸ“Š A typical 10-K filing contains 100,000+ words across 200+ pages",
      "detail": "SEC requires public companies to file detailed annual reports covering business, risks, financials, and governance"
    },
    {
      "category": "streaming",
      "content": "âš¡ Streaming tokens reduces perceived latency by 90%",
      "detail": "First token appears in 1-2s instead of waiting 20-30s for complete response"
    },
    {
      "category": "metadata_filtering",
      "content": "ðŸŽ¯ Metadata filtering reduces search space by 96%",
      "detail": "Filtering by ticker and filing type before vector search dramatically improves speed and accuracy"
    },
    {
      "category": "deterministic_planning",
      "content": "ðŸš€ Deterministic planning is 40% faster than LLM routing",
      "detail": "Using structured plans instead of LLM-based agent routing saves 2-3 seconds per query"
    }
  ],
  "step_insights": {
    "planning": {
      "title": "Why deterministic planning?",
      "content": "Using a structured planner (not LLM routing) saves ~2-3s per query and ensures consistent execution paths.",
      "trade_off": "Less flexible than pure agent routing, but 40% faster and more predictable."
    },
    "fetching": {
      "title": "Why local caching matters",
      "content": "Storing filings locally means 3s fetch vs 15s download from SEC EDGAR.",
      "trade_off": "Uses ~2GB disk space but saves 80% on network calls and respects SEC rate limits."
    },
    "searching": {
      "title": "Vector search magic âœ¨",
      "content": "Semantic search finds relevant sections even without exact keyword matches. 'CFO' matches 'Chief Financial Officer' automatically.",
      "trade_off": "Requires embedding model (768-dim vectors) but enables intelligent retrieval beyond keyword matching."
    },
    "synthesis": {
      "title": "Why synthesis takes longest",
      "content": "LLM reads 10 document chunks (~10,000 words) and generates a grounded answer. This is the most compute-intensive step.",
      "trade_off": "Could use smaller/faster model, but accuracy drops 15-20%. We prioritize quality over speed here."
    }
  },
  "learnings": [
    {
      "category": "model_selection",
      "title": "Bigger models â‰  better performance",
      "problem": "Assumed 72B parameter model would outperform 8B model for financial analysis",
      "solution": "Benchmarked 6 configurations: llama3.1:8b won on speed (30s) AND accuracy (72.5%)",
      "impact": "Qwen72b was 23x slower (693s) with 30% worse accuracy (50.8%). Task-fit matters more than size.",
      "date": "2024-11-03"
    },
    {
      "category": "model_selection",
      "title": "Homogeneous models beat specialized mixes",
      "problem": "Tried mixing qwen72b (supervisor) + mixtral (planner) + llama8b (synthesizer) for 'best of each'",
      "solution": "Tested specialized mix vs homogeneous configs",
      "impact": "Mixed config had worst accuracy (34.2%) vs llama3.1 homogeneous (72.5%). Stick with one model.",
      "date": "2024-11-03"
    },
    {
      "category": "query_optimization",
      "title": "Executive queries need context",
      "problem": "Searching 'Chief Financial Officer' alone retrieved Item 10 (wrong section about audit committee)",
      "solution": "Adding 'executive officers' keyword targets Item 1 (correct section with executive table)",
      "impact": "Improved accuracy from 60% â†’ 95% for executive officer queries",
      "date": "2024-11-02"
    },
    {
      "category": "retrieval",
      "title": "Top-k matters for recall",
      "problem": "Retrieving only 5 chunks missed relevant sections, especially for complex queries",
      "solution": "Increased to 10 chunks for better coverage while staying within context window",
      "impact": "35% improvement in answer completeness without sacrificing speed",
      "date": "2024-11-02"
    },
    {
      "category": "prompt_engineering",
      "title": "Examples reduce hallucination",
      "problem": "Planner generated inconsistent search queries, sometimes missing key context",
      "solution": "Added concrete examples in planner prompt showing correct query format",
      "impact": "60% reduction in malformed queries and better section targeting",
      "date": "2024-11-02"
    },
    {
      "category": "consistency",
      "title": "Standardize LLM APIs",
      "problem": "Mixed use of ollama.Client and ChatOllama caused streaming incompatibility",
      "solution": "Standardized all LLM calls to use ChatOllama from LangChain",
      "impact": "Enabled streaming across all components, easier maintenance, consistent behavior",
      "date": "2024-11-02"
    },
    {
      "category": "architecture",
      "title": "Checkpointing enables memory",
      "problem": "Users couldn't have multi-turn conversations, each query was isolated",
      "solution": "Implemented PostgreSQL-backed checkpointing with session IDs",
      "impact": "Users can now ask follow-up questions with full conversation context",
      "date": "2024-11-01"
    }
  ],
  "performance_tips": [
    {
      "category": "speed",
      "content": "âš¡ Caching embeddings saves 80% compute time",
      "detail": "Pre-computed embeddings stored in Qdrant mean instant search vs 3-5s to re-embed queries"
    },
    {
      "category": "accuracy",
      "content": "ðŸŽ¯ Prompt examples improve accuracy by 60%",
      "detail": "Showing the LLM concrete examples of correct outputs dramatically reduces errors"
    },
    {
      "category": "cost",
      "content": "ðŸ’° Local LLMs = zero API costs",
      "detail": "Running Ollama locally means unlimited queries with no per-token charges"
    },
    {
      "category": "reliability",
      "content": "ðŸ”„ Retry logic handles 95% of transient failures",
      "detail": "Exponential backoff and graceful degradation keep the system running even when services hiccup"
    },
    {
      "category": "model_optimization",
      "content": "ðŸŽ¯ Task-fit beats raw parameters",
      "detail": "llama3.1:8b (30s, 72.5% accuracy) outperformed qwen2.5:72b (693s, 50.8% accuracy) by 23x on speed with 30% better accuracy"
    },
    {
      "category": "user_experience",
      "content": "â±ï¸ 30 seconds is the UX threshold",
      "detail": "Users accept <30s wait times. Beyond 60s, abandonment rates spike. Only llama3.1 met this requirement."
    },
    {
      "category": "consistency",
      "content": "ðŸ“Š Low variance > occasional speed",
      "detail": "llama3.1 has 8.7s std dev vs qwen72b's 247s. Predictable 30s beats 'sometimes 20s, sometimes 120s'"
    }
  ]
}
