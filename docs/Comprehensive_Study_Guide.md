# Comprehensive Study Guide: FinanceAgent Project

Based on the docs folder, implementation, git history, and production learnings, here's an exhaustive list of concepts, technologies, decisions, and the **reasoning behind each choice** for interview preparation.

---

## üìö **PART 1: CORE CONCEPTS & ARCHITECTURE**

### **1.1 Multi-Agent Systems**
- [ ] **LangGraph Framework**
  - StateGraph vs MessageGraph
  - Nodes, edges, conditional edges
  - START and END constants
  - Graph compilation and execution
  - State management patterns
  - **Docs**: `docs/v2/01_FRAMEWORK_COMPARISON_AND_RECOMMENDATION.md`

- [ ] **Agent Design Patterns**
  - Orchestrator-Worker pattern
  - Supervisor-Agent architecture
  - Single responsibility principle for agents
  - Tool-calling agents vs reasoning agents
  - **Docs**: `docs/v2/02_MULTI_AGENT_ARCHITECTURE_DESIGN.md`

- [ ] **Framework Comparison**
  - LangGraph vs CrewAI vs AutoGen
  - Why LangGraph was chosen
  - Production readiness criteria
  - **Docs**: `docs/v2/01_FRAMEWORK_COMPARISON_AND_RECOMMENDATION.md`

### **1.2 RAG (Retrieval-Augmented Generation)**
- [ ] **RAG Pipeline Stages**
  - Query embedding
  - Vector search (cosine similarity)
  - Context building
  - LLM generation
  - Citation extraction
  - **Implementation**: `app/tools/rag_search_service.py`

- [ ] **Why RAG over Fine-Tuning**
  - Dynamic data updates (quarterly SEC filings)
  - Transparency (source citations)
  - Cost efficiency
  - Flexibility
  - **Docs**: `docs/Architecture.md:342-365`

---

## üìö **PART 2: CHECKPOINTING & SESSION MANAGEMENT**

### **2.1 LangGraph Checkpointing**
- [ ] **AsyncPostgresSaver**
  - Async context manager protocol
  - Manual lifecycle management (`__aenter__`, `__aexit__`)
  - Idempotent setup (table creation)
  - Singleton pattern for connection pooling
  - **Implementation**: `app/agents/supervisor.py:49-119`
  - **Docs**: `docs/v2/08_CHECKPOINT_AND_SESSION_MANAGEMENT.md`

- [ ] **Database Schema**
  - `checkpoints` table structure
  - `checkpoint_writes` table
  - `checkpoint_blobs` table
  - `checkpoint_migrations` table
  - Thread ID format: `{user_id}_{session_id}`
  - **Docs**: `docs/v2/08_CHECKPOINT_AND_SESSION_MANAGEMENT.md:725-753`

- [ ] **Issues Encountered**
  - "async context manager protocol" error (sync vs async)
  - "connection is closed" error (context manager lifecycle)
  - Race conditions in frontend (session loading)
  - **Docs**: `docs/v2/08_CHECKPOINT_AND_SESSION_MANAGEMENT.md:267-387`

### **2.2 Session Management**
- [ ] **Frontend localStorage**
  - Session ID persistence
  - Race condition prevention
  - New conversation handling
  - **Implementation**: `docs/v2/08_CHECKPOINT_AND_SESSION_MANAGEMENT.md:445-524`

- [ ] **Backend Thread Management**
  - Thread ID generation
  - RunnableConfig usage
  - Session continuity
  - **Implementation**: `app/agents/supervisor.py:230-307`

---

## üìö **PART 3: CONTEXT WINDOW MANAGEMENT**

### **3.1 Token-Based Trimming**
- [ ] **The Problem**
  - Unbounded conversation growth with checkpointing
  - Context window overflow (8,192 tokens for llama3.1:8b)
  - Variable message sizes (tool responses: 5,000-10,000 tokens)
  - **Docs**: `docs/Architecture.md:367-532`

- [ ] **Solution Options Compared**
  - Sliding window (message count) - REJECTED
  - Token-based trimming - CHOSEN
  - Conversation summarization - TOO COMPLEX
  - **Docs**: `docs/v2/00_START_HERE_ROADMAP.md:3-110`

- [ ] **Implementation**
  - LangChain's `trim_messages` utility
  - Token counter function
  - Max token budget (6,000 tokens)
  - Strategy: "last" (keep most recent)
  - **Implementation**: `app/agents/supervisor.py:153-178`

### **3.2 10 Benefits of Token Management**
- [ ] **Performance Benefits**
  - Inference speed: 3-4x faster (O(n¬≤) attention complexity)
  - Memory efficiency: 75% less VRAM
  - Predictable latency (consistent 4s vs 3-25s)
  - **Docs**: `docs/Architecture.md:535-845`

- [ ] **Cost & Scalability**
  - 76% cost reduction (for paid APIs)
  - 2.5x more concurrent users
  - Better token budget allocation
  - **Docs**: `docs/Architecture.md:666-712`

- [ ] **Quality Benefits**
  - Reduced hallucination risk
  - Fair resource allocation
  - Model compatibility
  - **Docs**: `docs/Architecture.md:635-781`

---

## üìö **PART 4: STRUCTURED OUTPUTS**

### **4.1 Pydantic Schemas**
- [ ] **Schema Definition**
  - `SynthesizerOutput` model
  - `PlannerOutput` model
  - Nested models (Answer, Section, CompanyData, Comparison)
  - **Implementation**: `app/schemas/synthesizer_output.py`

- [ ] **Ollama Structured Outputs**
  - `format` parameter with JSON schema
  - Constrained generation (grammar-based sampling)
  - Token-level constraints
  - Mathematical guarantee of valid JSON
  - **Implementation**: `app/tools/rag_search_service.py:222-230`

- [ ] **Benefits**
  - 10% ‚Üí 0% malformed JSON errors
  - 15% token reduction (removed formatting instructions)
  - Type safety with Pydantic
  - **Docs**: `docs/PRESENTATION_OUTLINE.md:872-957`

### **4.2 Prompt Engineering Decision**
- [ ] **Natural Text In ‚Üí JSON Out**
  - Context as natural text (not JSON)
  - Output as structured JSON
  - 20-30% token savings
  - LLMs comprehend narrative better
  - **Docs**: `docs/Architecture.md:977-1024`

---

## üìö **PART 5: MULTI-COMPANY COMPARISON**

### **5.1 The Simple Fix**
- [ ] **Data Structure Change**
  - Problem: `list` pooled all companies together
  - Solution: `dict` maintains per-company separation
  - Result: Equal retrieval (5 chunks per company)
  - **Implementation**: `app/tools/filing_qa_tool.py:442-518`
  - **Docs**: `docs/Architecture.md:848-975`

- [ ] **Key Learning**
  - Data structures > Complex architecture
  - Test before redesigning
  - 1-2 days vs 5-6 weeks
  - 95% less code than parallel agent approach
  - **Docs**: `docs/v2.1/COMPARATIVE_ANALYSIS_FEATURE_DECISION.md`

### **5.2 Hallucination Filtering**
- [ ] **Post-Processing**
  - Filter companies not in original context
  - Remove comparison data for single company
  - Validate against retrieved tickers
  - **Implementation**: `app/tools/filing_qa_tool.py:837-854`

---

## üìö **PART 6: MODEL SELECTION & BENCHMARKING**

### **6.1 Model Performance Analysis**
- [ ] **Benchmark Results**
  - llama3.1:8b: 30.2s, 72.5% accuracy (WINNER)
  - qwen2.5:72b: 692.9s, 50.8% accuracy (23x slower, worse!)
  - qwen2.5:32b: 197.9s, 51.7% accuracy
  - Mixtral:8x7b: FAILED (0/10 queries)
  - **Docs**: `docs/v2/11_MODEL_PERFORMANCE_ANALYSIS.md`

- [ ] **Key Findings**
  - Size ‚â† Performance (larger models worse)
  - Speed matters (only llama3.1 meets UX threshold <30s)
  - Consistency > Occasional speed
  - Mixed models don't work
  - **Docs**: `docs/v2/11_MODEL_PERFORMANCE_ANALYSIS.md:299-382`

### **6.2 Embedding Models**
- [ ] **nomic-embed-text-v1.5**
  - 768 dimensions (vs 1024 for BGE)
  - 8,192 token context window (vs 512 for BGE)
  - Native Ollama integration
  - 25% less memory than BGE-large
  - **Docs**: `docs/Architecture.md:76-94`

---

## üìö **PART 7: DETERMINISTIC vs LLM DECISIONS**

### **7.1 Deterministic Planning**
- [ ] **Why Deterministic?**
  - 40% faster than LLM routing
  - 100% reliable (no hallucinations)
  - Easier to debug
  - Lower cost (no LLM call)
  - **Implementation**: `app/tools/filing_qa_tool.py:180-281`

- [ ] **When to Use LLM**
  - Complex reasoning required
  - Natural language understanding needed
  - Synthesis and generation
  - **Pattern**: Planner (deterministic) ‚Üí Executor (deterministic) ‚Üí Synthesizer (LLM)

### **7.2 3-Stage Pipeline**
- [ ] **Stage 1: Planner (LLM)**
  - Query intent classification
  - Ticker extraction
  - Task generation
  - **Implementation**: `app/tools/filing_qa_tool.py:283-377`

- [ ] **Stage 2: Executor (Deterministic)**
  - Vector search
  - Data preparation
  - Chunk retrieval
  - **Implementation**: `app/tools/filing_qa_tool.py:442-518`

- [ ] **Stage 3: Synthesizer (LLM)**
  - Answer generation
  - Structured output
  - Citation formatting
  - **Implementation**: `app/tools/filing_qa_tool.py:524-777`

---

## üìö **PART 8: VECTOR DATABASE & EMBEDDINGS**

### **8.1 Qdrant Vector Store**
- [ ] **Configuration**
  - 768-dimensional vectors
  - Cosine similarity search
  - Metadata filtering (ticker, filing_type, section)
  - Score threshold (default: 0.5)
  - **Implementation**: `app/services/vector_store.py`

- [ ] **Why Qdrant?**
  - Performance optimized for similarity search
  - Rich metadata filtering
  - Scalability (millions of vectors)
  - Docker-friendly
  - **Docs**: `docs/Architecture.md:355-360`

### **8.2 Dual Storage Strategy**
- [ ] **PostgreSQL + Qdrant**
  - PostgreSQL: Metadata, full text, analytics
  - Qdrant: Vector search, semantic retrieval
  - Both store text (resilience, no cross-DB lookups)
  - **Docs**: `docs/Architecture.md:43-68`

---

## üìö **PART 9: CHUNKING STRATEGY**

### **9.1 Document Chunking**
- [ ] **Configuration**
  - Chunk size: 2,048 characters (production)
  - Overlap: 300 characters
  - RecursiveCharacterTextSplitter
  - **Docs**: `docs/Architecture.md:113-132`

- [ ] **Split Hierarchy**
  - Paragraph breaks (`\n\n`)
  - Line breaks (`\n`)
  - Sentence ends (`. `)
  - Word boundaries (` `)
  - Characters (last resort)

- [ ] **Why 2,048 characters?**
  - Leverages 25% of nomic's 8K context
  - Preserves financial tables intact
  - Reduces total chunks (better coherence)
  - Memory safe (~512 tokens per chunk)

---

## üìö **PART 10: PARSING & DATA PREPARATION**

### **10.1 SEC Filing Processing**
- [ ] **Pipeline Stages**
  - Download from SEC EDGAR
  - HTML parsing (BeautifulSoup4)
  - Section extraction
  - Table preservation
  - Chunking
  - Embedding generation
  - Storage (PostgreSQL + Qdrant)
  - **Implementation**: `app/tools/data_prep_service.py`

- [ ] **Idempotency**
  - Check if filing exists before processing
  - Avoid duplicate downloads
  - Safe to call repeatedly
  - **Implementation**: `app/tools/data_prep_service.py:199-248`

---

## üìö **PART 11: INFERENCE OPTIMIZATION**

### **11.1 Infrastructure Bottlenecks**
- [ ] **Docker GPU Passthrough**
  - macOS: No GPU support (CPU-only)
  - Linux: NVIDIA GPU support
  - Native Ollama on Mac: Metal GPU (6x faster)
  - **Docs**: `docs/PRESENTATION_OUTLINE.md:624-677`

### **11.2 vLLM Migration (Planned)**
- [ ] **Why vLLM?**
  - PagedAttention (efficient KV cache management)
  - Continuous batching (2-3x throughput)
  - Prefix caching
  - OpenAI-compatible API
  - **Docs**: `docs/v2.2/Inference_Improvement.md:19-104`

- [ ] **LMCache Integration (Planned)**
  - RAG-optimized caching
  - 3-5x TTFT reduction
  - CPU offloading for KV cache
  - Non-prefix caching
  - **Docs**: `docs/v2.2/Inference_Improvement.md:106-191`

### **11.3 Key Concepts**
- [ ] **PagedAttention Algorithm**
  - Treats KV cache like OS virtual memory
  - Paging mechanism
  - Efficient memory management
  - **Paper**: https://arxiv.org/abs/2309.06180

- [ ] **Continuous Batching**
  - Dynamic request batching
  - Add/remove requests on-the-fly
  - Higher GPU utilization
  - Lower latency

- [ ] **KV Cache Bottleneck**
  - Memory constraint in LLMs
  - 70B model: ~1GB per 1K tokens
  - Cache hierarchy: GPU ‚Üí CPU ‚Üí Disk

---

## üìö **PART 12: STREAMING & REAL-TIME UPDATES**

### **12.1 Server-Sent Events (SSE)**
- [ ] **Implementation**
  - Real-time log streaming
  - Progress tracking (planning ‚Üí fetching ‚Üí synthesis)
  - Token streaming
  - Source availability notifications
  - **Implementation**: `app/agents/supervisor.py:309-547`

- [ ] **Event Types**
  - `step_start`: New stage beginning
  - `tool_start`: Tool execution started
  - `tool_end`: Tool execution completed
  - `token`: Individual token (for text streaming)
  - `complete_structured`: Full structured response
  - `sources_ready`: Citations available
  - `error`: Error occurred

---

## üìö **PART 13: TESTING & EVALUATION**

### **13.1 Playwright Testing**
- [ ] **End-to-End Testing**
  - Browser automation
  - Real Chromium browser
  - Structured DOM snapshots
  - Integration testing (LLM ‚Üí Backend ‚Üí Frontend ‚Üí UI)
  - **Docs**: `docs/PLAYWRIGHT_TESTING_GUIDE.md`

- [ ] **Benefits**
  - 2-minute verification vs 5-minute manual testing
  - Catches integration bugs unit tests miss
  - Regression testing (just re-run)
  - **Docs**: `docs/PRESENTATION_OUTLINE.md:774-846`

### **13.2 Evaluation Strategy**
- [ ] **LLM-as-Judge**
  - Factual accuracy scoring
  - Citation quality
  - Completeness assessment
  - **Docs**: `docs/v2/04_TESTING_AND_EVALUATION_STRATEGY.md`

- [ ] **Metrics**
  - Success rate (10/10 queries)
  - Accuracy score (keyword matching)
  - Response time (P50, P95, P99)
  - Token usage

---

## üìö **PART 14: MONITORING & OBSERVABILITY**

### **14.1 Logging**
- [ ] **Structured Logging**
  - JSON format
  - Context: user_id, session_id, agent_name, step
  - Token usage tracking
  - Latency measurement
  - **Implementation**: Throughout codebase

- [ ] **Token Metrics**
  - Per-stage tracking (supervisor, planner, synthesizer)
  - Total tokens per query (~2,725 tokens)
  - Cost estimation
  - **Implementation**: `app/utils/token_metrics.py`

### **14.2 Metrics to Track**
- [ ] **Performance**
  - Average response time
  - Trimming frequency
  - Max tokens seen
  - P95 latency

- [ ] **Quality**
  - Accuracy rate
  - Hallucination detection
  - Citation completeness

- [ ] **Cost**
  - Tokens per query
  - API costs (if using paid LLMs)
  - GPU utilization

---

## üìö **PART 15: DEPLOYMENT & INFRASTRUCTURE**

### **15.1 Docker Compose**
- [ ] **Services**
  - PostgreSQL (metadata + checkpoints)
  - Qdrant (vector search)
  - Ollama (LLM inference)
  - Redis (caching)
  - Backend (FastAPI)
  - Frontend (React + Nginx)
  - **Config**: `docker-compose.prod.yml`

### **15.2 Production Considerations**
- [ ] **Scaling**
  - Horizontal scaling (multiple uvicorn workers)
  - GPU server for Ollama
  - Managed databases (RDS, Qdrant Cloud)
  - **Docs**: `docs/DEPLOYMENT.md`

- [ ] **Security**
  - API key authentication
  - Rate limiting
  - CORS configuration
  - Environment variable secrets
  - **Docs**: `docs/SECURITY.md`

---

## üìö **PART 16: PROMPT ENGINEERING**

### **16.1 Prompt Organization**
- [ ] **Separation of Concerns**
  - Prompts in separate files (`prompts/` directory)
  - Never hardcode in Python
  - Version control alongside code
  - **Pattern**: `Path(__file__).parent.parent / "prompts" / "supervisor.txt"`

### **16.2 Prompt Techniques**
- [ ] **Few-Shot Examples**
  - Concrete examples in planner prompt
  - 60% reduction in malformed queries
  - **Implementation**: Planner prompt file

- [ ] **Structured Instructions**
  - Clear output format
  - Negative examples (what NOT to do)
  - XML tags for parsing
  - Citation requirements

---

## üìö **PART 17: ERROR HANDLING & RESILIENCE**

### **17.1 Graceful Degradation**
- [ ] **Missing Data Handling**
  - Explain what's missing
  - Suggest alternatives
  - Partial results better than nothing
  - **Implementation**: Throughout tools

- [ ] **Retry Logic**
  - Exponential backoff
  - Circuit breakers
  - Timeout handling
  - **Pattern**: Wrap tool calls in try-except

### **17.2 JSON Parsing Recovery**
- [ ] **Malformed JSON Handling**
  - Attempt repair (add closing braces)
  - Extract content fields with regex
  - Fallback to readable text
  - **Implementation**: `app/tools/filing_qa_tool.py:608-777`

---

## üìö **PART 18: CONFIGURATION MANAGEMENT**

### **18.1 Environment Variables**
- [ ] **Required Settings**
  - `DATABASE_URL`: PostgreSQL connection
  - `SEC_USER_AGENT`: Email for SEC API
  - `OLLAMA_BASE_URL`: LLM endpoint
  - **Config**: `app/core/config.py`

- [ ] **Model Configuration**
  - `SUPERVISOR_MODEL`: llama3.1:8b
  - `PLANNER_MODEL`: llama3.1:8b
  - `SYNTHESIZER_MODEL`: llama3.1:8b
  - `EMBEDDING_MODEL`: nomic-embed-text-v1.5

- [ ] **RAG Settings**
  - `CHUNK_SIZE`: 2048
  - `CHUNK_OVERLAP`: 300
  - `TOP_K`: 5
  - `SCORE_THRESHOLD`: 0.5
  - `MAX_CONVERSATION_TOKENS`: 6000

---

## üìö **PART 19: LLM PROVIDER ABSTRACTION**

### **19.1 LLM Factory Pattern**
- [ ] **Dual Provider Support**
  - Ollama (local)
  - vLLM (production)
  - OpenAI-compatible API
  - **Implementation**: `app/utils/llm_factory.py`

- [ ] **Configuration**
  - `LLM_PROVIDER`: "ollama" or "vllm"
  - Automatic endpoint switching
  - Structured output support (Ollama only)

---

## üìö **PART 20: KEY LEARNINGS & DECISIONS**

### **20.1 Architecture Decisions**
- [ ] **Simplicity Wins**
  - Data structure change > Complex architecture
  - Deterministic > LLM where possible
  - Test before redesigning
  - **Docs**: `docs/Architecture.md:848-975`

- [ ] **Performance Optimization**
  - Infrastructure first (GPU access)
  - Token management (O(n¬≤) complexity)
  - Caching strategies
  - **Docs**: Multiple locations

### **20.2 Production Lessons**
- [ ] **Data Safety**
  - Never recommend destructive commands
  - Always provide backup steps
  - Warn about consequences
  - **Docs**: `docs/DATA_SAFETY_RULES.md`

- [ ] **Incremental Development**
  - Ship fast, iterate
  - 90% solution > 100% delayed
  - User feedback drives features

---

## üìö **PART 21: DECISION RATIONALES (WHY, NOT JUST WHAT)**

### **21.1 Token-Based Trimming vs Sliding Window**

**Context**: Turn 10+ conversations caused context overflow (8,192 tokens for llama3.1:8b)

| Option | Pros | Cons | Verdict |
|--------|------|------|---------|
| **Sliding Window (Message Count)** | Simple, fast, predictable | 8 messages could still be 20,000+ tokens. No guarantee. Variable tool responses (5K-10K tokens) break assumption. | ‚ùå REJECTED |
| **Token-Based Trimming** | Hard guarantee via `trim_messages`. Battle-tested LangChain utility. Adaptive to message sizes. | Slightly more complex than message counting | ‚úÖ CHOSEN |
| **Conversation Summarization** | Preserves all context | Extra LLM call, added latency, overkill for independent financial queries | ‚ùå REJECTED |

**Result**: 6,000 token budget (leaves 2,000 for response). O(n¬≤) attention scales predictably. 3-4x faster on long conversations.

**Implementation**: `app/agents/supervisor.py:153-178`

---

### **21.2 dict vs list for Multi-Company Comparison**

**Problem**: "Compare AAPL vs MSFT" returned imbalanced results (7 AAPL chunks, 3 MSFT chunks).

**Root Cause Analysis**:
```python
# BEFORE: list pooled all results
all_chunks = []  # Combined pool
for ticker in tickers:
    chunks = search(ticker)
    all_chunks.extend(chunks)  # Apple dominates if more data exists

# AFTER: dict maintains separation
chunks_by_company = {}  # Per-company buckets
for ticker in tickers:
    chunks_by_company[ticker] = search(ticker)[:5]  # 5 each, guaranteed
```

**Alternative Considered**: Parallel Agent Architecture (5-6 weeks)
- Dedicated RAG agent per company
- State aggregation complexity
- LangGraph subgraph patterns

**Why Simple Fix Won**:
- 1-2 days vs 5-6 weeks
- 95% less code
- Same result quality
- **Lesson**: Test before redesigning

**Docs**: `docs/v2.1/COMPARATIVE_ANALYSIS_FEATURE_DECISION.md`

---

### **21.3 LangGraph vs CrewAI vs AutoGen**

| Criteria | LangGraph | CrewAI | AutoGen |
|----------|-----------|--------|---------|
| **State Management** | ‚úÖ Built-in StateGraph | ‚ö†Ô∏è Manual | ‚ö†Ô∏è Session-based |
| **Checkpointing** | ‚úÖ Native PostgresSaver | ‚ùå None | ‚ùå None |
| **Debugging** | ‚úÖ LangSmith, graph viz | ‚ö†Ô∏è Basic logs | ‚ö†Ô∏è Basic logs |
| **Production Ready** | ‚úÖ Used by Anthropic | ‚ö†Ô∏è Growing | ‚ö†Ô∏è Research focus |
| **Learning Curve** | Medium | Low | High |
| **Control** | ‚úÖ Full graph control | ‚ö†Ô∏è Opinionated | ‚ö†Ô∏è Complex |

**Decision**: LangGraph for production-grade state management and checkpointing.

**Pain Point Avoided**: CrewAI's lack of native persistence would require custom checkpoint implementation.

---

### **21.4 Deterministic Planning vs LLM Routing**

**Question**: Should the Planner use an LLM to decide what to retrieve?

| Approach | Latency | Reliability | Cost | Debuggability |
|----------|---------|-------------|------|---------------|
| **LLM Routing** | +2-3s | 85-95% | $$$ | Hard |
| **Deterministic (regex/rules)** | <100ms | 100% | Free | Easy |

**Pattern Adopted**: 
```
Planner (LLM) ‚Üí Executor (Deterministic) ‚Üí Synthesizer (LLM)
```

- Planner extracts tickers and intent (needs NLU)
- Executor does vector search (no LLM needed)
- Synthesizer generates answer (needs reasoning)

**Result**: 40% faster, zero routing failures.

---

### **21.5 Qdrant vs PGVector**

**Initial Choice**: Qdrant (separate vector DB)
- Rich metadata filtering
- Specialized performance tuning
- Docker-native

**Reconsidered in sec_parser branch**: PGVector (single DB)
- JOINs between vectors and metadata
- No cross-DB latency
- Simpler ops (one DB to maintain)
- Transactional consistency

**Tradeoff**: Qdrant offers more advanced features (quantization, distributed sharding). PGVector simpler for single-node deployments.

**Current State**: Production uses Qdrant. sec_parser branch (stashed) migrates to PGVector.

---

### **21.6 nomic-embed-text vs BGE-large**

| Feature | nomic-embed-text | BGE-large-en-v1.5 |
|---------|------------------|-------------------|
| **Dimensions** | 768 | 1024 |
| **Context Window** | 8,192 tokens | 512 tokens |
| **Memory** | Lower | 25% higher |
| **Integration** | Native Ollama | sentence-transformers |

**Decision**: nomic-embed-text for 16x longer context window.

**Impact**: Financial tables (often >512 tokens) embed properly without truncation.

---

## üìö **PART 22: FAILURE MODES & RECOVERY**

### **22.1 LLM Returns Malformed JSON**

**Detection**: Pydantic validation fails

**Recovery Chain**:
1. **JSON Repair**: Add missing closing braces/brackets
   ```python
   def repair_json(text):
       open_braces = text.count('{') - text.count('}')
       return text + '}' * open_braces
   ```
2. **Regex Extraction**: Pull `"answer"` field directly
3. **Raw Text Fallback**: Return unstructured response

**Prevention**: Ollama structured outputs with `format` parameter (grammar-based sampling).

**Implementation**: `app/tools/filing_qa_tool.py:608-777`

---

### **22.2 Vector Search Returns Empty Results**

**Causes**:
- Filing not processed yet
- Query too vague
- Score threshold too high

**Recovery**:
1. Check if filing exists; if not, trigger processing
2. Lower score threshold from 0.5 ‚Üí 0.3
3. Expand search to related sections
4. Return "No relevant information found" with suggestion

**User Message**:
> "I couldn't find relevant information about [topic] in [ticker]'s filings. Try being more specific or asking about a different topic."

---

### **22.3 SEC API Rate Limit (429)**

**Detection**: HTTP 429 status code

**Recovery**:
```python
async def fetch_with_retry(url, max_retries=3):
    for attempt in range(max_retries):
        response = await fetch(url)
        if response.status == 429:
            wait = 2 ** attempt  # 2s, 4s, 8s
            await asyncio.sleep(wait)
            continue
        return response
    raise SECRateLimitError("Max retries exceeded")
```

**SEC Requirements**:
- User-Agent must include email
- Max 10 requests/second
- **Docs**: `docs/SECURITY.md`

---

### **22.4 Checkpoint Corruption**

**Symptoms**: 
- "connection is closed" errors
- State not persisting
- Duplicate messages

**Recovery**:
1. Verify checkpointer initialization (startup)
2. Clear corrupted session: `DELETE FROM checkpoints WHERE thread_id = ?`
3. Restart with new session_id
4. Add database connection pooling

**Prevention**: 
- Initialize checkpointer in lifespan context
- Use singleton pattern for connection reuse

---

### **22.5 Embedding Service Timeout**

**Context**: 500 chunks √ó 0.1s/chunk = 50s total

**Detection**: Timeout after 60s

**Recovery**:
1. Batch embeddings (100 chunks per batch)
2. Retry failed batches only
3. Store partial progress to resume

**User Experience**: Show progress bar ("Embedding chunk 100/500...")

---

## üìö **PART 23: SEC_PARSER ARCHITECTURE EVOLUTION (V2.3)**

### **23.1 The Problem with Single-Parser Approach**

**Original Design**: BeautifulSoup HTML parser for everything

**Limitations**:
- **Quantitative Data**: Exact numbers buried in HTML tables, hard to extract reliably
- **Vector Dilution**: Large tables embed as single chunk; specific rows (e.g., "Mac revenue") lost in noise
- **No XBRL**: Structured financial data ignored

---

### **23.2 Bifurcated Truth Architecture**

**Philosophy**: Different data types need different parsers.

| Data Type | Parser | Storage | Use Case |
|-----------|--------|---------|----------|
| **Quantitative** (exact numbers) | EdgarTools (XBRL) | `financial_facts` table | "What was revenue?" |
| **Qualitative** (narrative) | sec-parser (semantic HTML) | `semantic_chunks` table | "Summarize risk factors" |

**XBRL Facts Example**:
```python
{
  "tag": "us-gaap:RevenueFromContractWithCustomer",
  "value": 307003000000.0,
  "period_end": "2025-09-27",
  "segment_label": "iPhone",
  "unit": "USD"
}
```

**Semantic Chunks Example**:
```
Section: "Item 1A > Technology Risks > Cybersecurity"
Text: "We face risks from sophisticated cyber attacks..."
Type: "text"
```

---

### **23.3 New Components (sec_parser branch)**

| Component | Purpose | File |
|-----------|---------|------|
| **EdgarToolsParser** | Extract XBRL facts from financial statements | `app/services/edgartools_parser.py` |
| **SecParserParser** | Create semantic chunks with hierarchy | `app/services/secparser_parser.py` |
| **HTMLSectionParser** | Alternative BeautifulSoup-based parser | `app/services/html_section_parser.py` |
| **ReRanker** | Cross-encoder re-ranking for precision | `app/services/reranker.py` |

---

### **23.4 Vector Dilution Solution**

**Problem**: "Mac revenue" query returns generic Apple revenue chunk (Rank #4) because dominant terms ("iPhone") skewed embedding.

**Solution**: Parent-Child Indexing + Hybrid Search

1. **Row-Level Embedding**: Embed each table row, not full table
2. **Keyword Boosting**: Re-rank based on query term overlap
3. **Oversampling**: Retrieve Top 20, re-rank to Top 5

**Result**: +17% vector similarity. "Mac revenue" moved from Rank #4 to Rank #1.

---

### **23.5 Data Coverage (AAPL 10-K)**

**EdgarToolsParser Output**:
- 1,131 facts from 9 statement types
- Income Statement: 180 facts
- Balance Sheet: 182 facts
- Cash Flow: 90 facts

**SecParserParser Output**:
- 459 chunks from 156 sections
- Text chunks: 248
- Heading chunks: 164
- **Table chunks: 47**

**Coverage**: 100% of Items 1-16, all financial statements.

---

### **23.6 Current Status**

**Branch**: `sec_parser` (stashed)

**Why Stashed**: 
- Major refactor, needs thorough testing
- Production stable on current architecture
- Will complete after vLLM migration

---

## üìö **PART 24: COST ANALYSIS**

### **24.1 Token Usage Per Query Type**

| Query Type | Supervisor | Planner | Synthesizer | Total |
|------------|------------|---------|-------------|-------|
| **Single Company** | ~200 | ~300 | ~2,000 | ~2,500 |
| **Multi-Company (2)** | ~200 | ~400 | ~3,500 | ~4,100 |
| **Multi-Company (3)** | ~200 | ~500 | ~5,000 | ~5,700 |

**Average**: ~2,725 tokens per query

---

### **24.2 Local Inference (Ollama)**

| Resource | Cost |
|----------|------|
| **GPU (Mac M1/M2)** | $0 (owned hardware) |
| **Electricity** | ~$0.05/hour |
| **Per Query** | ~$0.001 |

---

### **24.3 Cloud GPU (If Scaling)**

| Provider | Instance | $/hour | Queries/hour | $/query |
|----------|----------|--------|--------------|---------|
| **RunPod** (A10G) | 24GB | $0.74 | ~200 | $0.004 |
| **Lambda Labs** (A100) | 80GB | $1.10 | ~500 | $0.002 |
| **AWS** (g5.xlarge) | 24GB | $1.01 | ~200 | $0.005 |

---

### **24.4 API Comparison (If Using OpenAI/Anthropic)**

| Model | Input $/1K | Output $/1K | Per Query |
|-------|------------|-------------|-----------|
| **GPT-4 Turbo** | $0.01 | $0.03 | ~$0.08 |
| **Claude 3 Opus** | $0.015 | $0.075 | ~$0.20 |
| **Local Ollama** | $0 | $0 | ~$0.001 |

**Savings**: 80-200x cheaper with local inference.

---

### **24.5 Embedding Costs**

| Filing | Chunks | Embedding Time | One-Time Cost |
|--------|--------|----------------|---------------|
| **10-K** | ~1,200 | ~50s | $0 (local) |
| **10-Q** | ~400 | ~20s | $0 (local) |

**Storage**: 768-dim √ó 4 bytes √ó 1,200 chunks = ~3.7 MB per 10-K

---

## üìö **PART 25: OPERATIONAL RUNBOOK**

### **25.1 Debugging Slow Queries**

**Symptom**: Query takes >30 seconds

**Diagnostic Steps**:
1. Check debug panel for stage timing
2. Identify bottleneck:
   - **Fetching**: SEC API slow ‚Üí check rate limits
   - **Embedding**: GPU not available ‚Üí verify Ollama GPU access
   - **LLM Generation**: Context too large ‚Üí check token count

**Quick Fix**: Enable Metal GPU on Mac
```bash
# Native Ollama (not Docker) for Mac
OLLAMA_GPU=metal ollama serve
```

---

### **25.2 Recovering from Checkpoint Issues**

**Symptom**: Old messages reappearing, state not saving

**Resolution**:
```sql
-- Clear corrupted session
DELETE FROM checkpoints WHERE thread_id LIKE '%{session_id}%';
DELETE FROM checkpoint_writes WHERE thread_id LIKE '%{session_id}%';
DELETE FROM checkpoint_blobs WHERE thread_id LIKE '%{session_id}%';
```

**Prevention**: Verify checkpointer initialization in startup logs.

---

### **25.3 Handling Missing Filings**

**Symptom**: "No filings found for TICKER"

**Diagnostic**:
```sql
SELECT * FROM filings WHERE ticker = 'TICKER';
```

**Resolution**: Manually trigger processing
```bash
curl -X POST http://localhost:8000/api/companies/TICKER/process \
  -H "Content-Type: application/json" \
  -d '{"filing_type": "10-K"}'
```

---

### **25.4 Memory Leak Detection**

**Symptom**: Backend memory grows over time

**Diagnostic**:
```bash
docker stats financeagent_backend
```

**Common Causes**:
- Unbounded message history (fixed with token trimming)
- Cache not evicting
- Connection pool exhaustion

**Resolution**: Restart backend, add memory limits in Docker Compose.

---

## üìö **PART 26: INTERVIEW TALKING POINTS**

### **26.1 "Tell me about a challenging bug"**

> "Early on, users reported the system crashing after 10-15 conversation turns. Debugging showed context window overflow‚Äîwe were passing full conversation history to the LLM. Tool responses are 5,000-10,000 tokens each, so 10 turns could mean 50,000+ tokens against an 8,192 limit.
>
> I analyzed three solutions: sliding window (message count), token-based trimming, and summarization. Sliding window couldn't handle variable message sizes. Summarization added latency. LangChain's `trim_messages` gave a mathematical guarantee with its token counter.
>
> Result: 6,000 token budget, 3-4x faster on long conversations, zero overflow crashes."

---

### **26.2 "What would you do differently?"**

> "Two things: First, I'd start with PGVector instead of Qdrant. Having a separate vector database adds operational complexity. PostgreSQL's pgvector extension enables JOINs between metadata and vectors in a single query.
>
> Second, I'd invest in better evaluation earlier. We had subjective quality assessments for too long. Now we have keyword-overlap scoring and citation verification, but having an LLM-as-judge eval suite from day one would have caught issues faster."

---

### **26.3 "How do you evaluate LLM output quality?"**

> "Three layers: First, **citation verification**‚Äîevery factual claim must reference a retrieved chunk. Second, **hallucination filtering**‚Äîif the LLM mentions a company not in the retrieved context, we strip it. Third, **keyword overlap scoring**‚Äîdoes the answer contain expected terms like 'revenue', 'growth', specific dollar amounts?
>
> For regression testing, we run 10 canonical queries and compare answers to expected patterns."

---

### **26.4 "Walk me through a query"**

> "User asks 'Compare Apple and Microsoft revenue.' 
> 1. **Supervisor** receives query, binds the FilingQATool. 
> 2. **Planner** (LLM) extracts tickers [AAPL, MSFT] and classifies as comparison. 
> 3. **Executor** (deterministic) runs vector search: 5 chunks per company, stored in a dict for even distribution. 
> 4. **Synthesizer** (LLM) generates structured JSON with `answer`, `companies`, `comparison` fields. 
> 5. Frontend renders with citations and expandable sources.
>
> Key insight: We moved from a parallel agent design to a simple dict-based approach. Same quality, 95% less code, 5 weeks faster delivery."

---

### **26.5 "What's the hardest part of multi-agent systems?"**

> "State coordination. In multi-company queries, we initially used a list to pool retrieved chunks. Apple had more data, so it dominated (7 chunks vs 3 for Microsoft). 
>
> The fix was trivially simple: use a dict keyed by ticker, enforcing 5 chunks each. But the *insight* required understanding how vector search ranking interacts with data imbalance.
>
> The broader lesson: before designing complex parallel agent architectures, test if a data structure change solves the problem."

---

### **26.6 "Why local LLM instead of GPT-4?"**

> "Three reasons: cost, latency, and data privacy. 
> - **Cost**: Local inference is 80-200x cheaper than API calls.
> - **Latency**: No network round-trip. 3-5 seconds vs 8-10 seconds.
> - **Privacy**: SEC data is public, but user queries might reveal investment strategies.
>
> Tradeoff: Model quality. We benchmarked 4 models. Surprisingly, llama3.1:8b beat qwen2.5:72b on our financial Q&A tasks. Bigger isn't always better‚Äîdomain fit matters."

---

## üéØ **STUDY PRIORITY MATRIX**

### **Critical (Must Know for Interviews)**
1. ‚úÖ Multi-agent architecture (LangGraph)
2. ‚úÖ RAG pipeline (embedding ‚Üí search ‚Üí generation)
3. ‚úÖ Checkpointing (AsyncPostgresSaver)
4. ‚úÖ Context window management (token trimming)
5. ‚úÖ Structured outputs (Pydantic + Ollama)
6. ‚úÖ Model benchmarking results
7. ‚úÖ Data structure decisions (dict vs list)
8. ‚úÖ **Decision rationales (why, not just what)**

### **Important (Deep Understanding)**
9. ‚úÖ Deterministic vs LLM decisions
10. ‚úÖ Prompt engineering patterns
11. ‚úÖ Vector databases (Qdrant/PGVector)
12. ‚úÖ Chunking strategies
13. ‚úÖ Error handling & resilience
14. ‚úÖ Streaming (SSE)
15. ‚úÖ Session management
16. ‚úÖ **Failure modes & recovery**

### **Good to Know (Production Skills)**
17. ‚úÖ Docker deployment
18. ‚úÖ Monitoring & observability
19. ‚úÖ Testing strategies (Playwright)
20. ‚úÖ Security considerations
21. ‚úÖ Infrastructure optimization (GPU)
22. ‚úÖ vLLM & LMCache (future)
23. ‚úÖ **Cost analysis**
24. ‚úÖ **Operational runbook**

---

## üìñ **RECOMMENDED READING ORDER**

1. **Start Here**: `docs/v2/00_START_HERE_ROADMAP.md`
2. **Architecture**: `docs/Architecture.md`
3. **Framework Choice**: `docs/v2/01_FRAMEWORK_COMPARISON_AND_RECOMMENDATION.md`
4. **Checkpointing**: `docs/v2/08_CHECKPOINT_AND_SESSION_MANAGEMENT.md`
5. **Model Analysis**: `docs/v2/11_MODEL_PERFORMANCE_ANALYSIS.md`
6. **Comparison Feature**: `docs/v2.1/COMPARATIVE_ANALYSIS_FEATURE_DECISION.md`
7. **Inference Optimization**: `docs/v2.2/Inference_Improvement.md`
8. **Presentation**: `docs/PRESENTATION_OUTLINE.md`

---

## üîó **EXTERNAL RESOURCES TO STUDY**

### **Papers**
- vLLM: https://arxiv.org/abs/2309.06180
- CacheBlend: https://dl.acm.org/doi/10.1145/3689031.3696098
- Attention is All You Need: https://arxiv.org/abs/1706.03762

### **Documentation**
- LangGraph: https://langchain-ai.github.io/langgraph/
- Anthropic Multi-Agent: https://www.anthropic.com/engineering/multi-agent-research-system
- Ollama: https://ollama.ai/
- Qdrant: https://qdrant.tech/documentation/
- PGVector: https://github.com/pgvector/pgvector

### **Blogs**
- vLLM Blog: https://blog.vllm.ai/
- LangChain Blog: https://blog.langchain.com/

---

## üìÖ **PROJECT TIMELINE**

| Phase | Duration | Key Deliverable |
|-------|----------|-----------------|
| **V1: Basic RAG** | Week 1-2 | Single-company Q&A working |
| **V1.5: Multi-Agent** | Week 3-4 | Supervisor + FilingQATool |
| **V2: Checkpointing** | Week 5-6 | Session persistence, streaming |
| **V2.1: Comparison** | Week 7-8 | Multi-company analysis |
| **V2.2: Optimization** | Week 9-10 | Token trimming, structured outputs |
| **V2.3: sec_parser** | Week 11+ | Dual parser, PGVector (stashed) |

---

This study guide covers **every major concept, decision, implementation detail, and the reasoning behind each choice** in your FinanceAgent project. Use it as a comprehensive reference for refreshing everything you've built and for interview preparation!